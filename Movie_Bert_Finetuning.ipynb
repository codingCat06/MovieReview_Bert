{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 영화 리뷰 감성 분석 Fine-tuning\n",
    "\n",
    "이 노트북은 기존 Movie_Bert.ipynb와 같은 모델과 데이터를 사용하되, fine-tuning을 추가로 수행합니다.\n",
    "\n",
    "## 기존 방식과의 차이점:\n",
    "- 동일한 모델: BertTokenizer, BertModel (\"monologg/koelectra-base-v3-discriminator\")\n",
    "- 동일한 데이터: NSMC 데이터셋\n",
    "- **추가 사항**: End-to-end fine-tuning을 통한 성능 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 디바이스: cpu\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 import (ELECTRA 모델 사용으로 수정)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (ElectraTokenizer, ElectraModel, ElectraForSequenceClassification, \n",
    "                         get_linear_schedule_with_warmup)\n",
    "from torch.optim import AdamW  # ✅ PyTorch에서 가져오기\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 모델 로드 완료!\n",
      "토크나이저 어휘 크기: 35000\n"
     ]
    }
   ],
   "source": [
    "# ELECTRA 모델 사용으로 수정 (BertTokenizer -> ElectraTokenizer, BertModel -> ElectraModel)\n",
    "from transformers import ElectraTokenizer, ElectraModel\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    " \n",
    "model = ElectraModel.from_pretrained(\n",
    "    \"monologg/koelectra-base-v3-discriminator\", \n",
    "    output_hidden_states=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "print(\"ELECTRA 모델 로드 완료!\")\n",
    "print(f\"토크나이저 어휘 크기: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 데이터:\n",
      "Train: 150000, Test: 50000\n",
      "         id                                           document  label\n",
      "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
      "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
      "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
      "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
      "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
      "\n",
      "NaN 값 제거 전 - Train: 150000, Test: 50000\n",
      "NaN 값 제거 후 - Train: 149995, Test: 49997\n",
      "\n",
      "최종 샘플 크기 - Train: 6000, Test: 2000\n",
      "샘플 데이터:\n",
      "        id                    document  label\n",
      "0  7865795                      원본이 최고      1\n",
      "1  5417631            스릴감과 훈훈함이 있는 영화.      1\n",
      "2  8357466      굉장히 저평가되는 영화중 하나라고 생각함      1\n",
      "3  8252946  정말영화같은이야기 영화여서 영화같은이야기가 좋다      1\n",
      "4  7800452                 계기도없는데 이상하다      0\n"
     ]
    }
   ],
   "source": [
    "train_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
    "train_df = pd.read_csv(train_url, sep=\"\\t\")\n",
    "\n",
    "test_url = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "test_df = pd.read_csv(test_url, sep=\"\\t\")\n",
    "\n",
    "print(\"원본 데이터:\")\n",
    "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "print(train_df.head())\n",
    "\n",
    "# NaN 값 제거 (중요: 인코딩 전에 먼저 제거)\n",
    "print(f\"\\nNaN 값 제거 전 - Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "train_df = train_df.dropna().reset_index(drop=True)\n",
    "test_df = test_df.dropna().reset_index(drop=True)\n",
    "print(f\"NaN 값 제거 후 - Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# 샘플 추출\n",
    "train_df = train_df.sample(6000, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(2000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n최종 샘플 크기 - Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "print(\"샘플 데이터:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 데이터 인코딩 중: 100%|██████████| 6000/6000 [31:33<00:00,  3.17it/s]\n",
      "Test 데이터 인코딩 중: 100%|██████████| 2000/2000 [10:30<00:00,  3.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_encodings = []\n",
    "for i, text in enumerate(tqdm(train_df['document'], desc=\"Train 데이터 인코딩 중\")):\n",
    "    # NaN 체크 제거 (이미 전처리에서 제거됨)\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding='max_length', max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    train_encodings.append(outputs.pooler_output.squeeze().numpy())\n",
    "\n",
    "test_encodings = []\n",
    "for i, text in enumerate(tqdm(test_df['document'], desc=\"Test 데이터 인코딩 중\")):\n",
    "    # NaN 체크 제거 (이미 전처리에서 제거됨)\n",
    "    inputs = tokenizer(str(text), return_tensors=\"pt\", truncation=True, padding='max_length', max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    test_encodings.append(outputs.pooler_output.squeeze().numpy())\n",
    "\n",
    "# 크기 확인\n",
    "print(f\"\\n인코딩 결과:\")\n",
    "print(f\"Train 인코딩: {len(train_encodings)}개 (DF: {len(train_df)}개)\")\n",
    "print(f\"Test 인코딩: {len(test_encodings)}개 (DF: {len(test_df)}개)\")\n",
    "print(f\"임베딩 차원: {train_encodings[0].shape if train_encodings else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5999\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [5999, 6000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_encodings))\n\u001b[0;32m      3\u001b[0m logistic \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mlogistic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m preds \u001b[38;5;241m=\u001b[39m logistic\u001b[38;5;241m.\u001b[39mpredict(test_encodings)\n\u001b[0;32m      6\u001b[0m baseline_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], preds)\n",
      "File \u001b[1;32mc:\\Users\\CodingCat\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\CodingCat\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1220\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1222\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\CodingCat\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\CodingCat\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\utils\\validation.py:1389\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1371\u001b[0m     X,\n\u001b[0;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1385\u001b[0m )\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1389\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\CodingCat\\anaconda3\\envs\\torch_env\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [5999, 6000]"
     ]
    }
   ],
   "source": [
    "# 로지스틱 회귀로 분류\n",
    "print(len(train_encodings))\n",
    "logistic = LogisticRegression(max_iter=1000)\n",
    "logistic.fit(train_encodings, train_df['label'])\n",
    "preds = logistic.predict(test_encodings)\n",
    "baseline_accuracy = accuracy_score(test_df['label'], preds)\n",
    "print(f\"\\n기존 방식 정확도: {baseline_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning을 위한 데이터셋 클래스 정의\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "        \n",
    "        # 토크나이징 (기존 방식과 동일)\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"NSMCDataset 클래스 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning용 ELECTRA 모델 설정\n",
    "print(\"=== Fine-tuning용 ELECTRA 모델 설정 ===\")\n",
    "\n",
    "# ElectraForSequenceClassification 모델 로드\n",
    "fine_tune_model = ElectraForSequenceClassification.from_pretrained(\n",
    "    \"monologg/koelectra-base-v3-discriminator\",\n",
    "    num_labels=2,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "fine_tune_model.to(device)\n",
    "\n",
    "# 데이터 준비 (더 작은 샘플 사용으로 빠른 실험)\n",
    "train_subset = train_df.sample(n=2000, random_state=42)\n",
    "test_subset = test_df.sample(n=800, random_state=42)\n",
    "\n",
    "# 데이터로더 생성\n",
    "train_dataset = NSMCDataset(train_subset['document'], train_subset['label'], tokenizer)\n",
    "test_dataset = NSMCDataset(test_subset['document'], test_subset['label'], tokenizer)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Fine-tuning 데이터:\")\n",
    "print(f\"- 훈련: {len(train_subset)}개\")\n",
    "print(f\"- 테스트: {len(test_subset)}개\")\n",
    "print(f\"- 배치 크기: {batch_size}\")\n",
    "print(f\"- 모델 유형: ELECTRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning 설정\n",
    "epochs = 2\n",
    "learning_rate = 2e-5\n",
    "\n",
    "optimizer = AdamW(fine_tune_model.parameters(), lr=learning_rate, eps=1e-8)\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuning 설정:\")\n",
    "print(f\"- 에폭: {epochs}\")\n",
    "print(f\"- 학습률: {learning_rate}\")\n",
    "print(f\"- 총 스텝: {total_steps}\")\n",
    "print(f\"- 모델 파라미터 수: {sum(p.numel() for p in fine_tune_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 평가 함수 정의\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += torch.sum(predictions == labels).item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        current_accuracy = correct_predictions / total_predictions\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{current_accuracy:.4f}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(data_loader), correct_predictions / total_predictions\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct_predictions += torch.sum(predictions == labels).item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return avg_loss, accuracy, all_predictions, all_labels\n",
    "\n",
    "print(\"훈련 및 평가 함수 정의 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning 실행\n",
    "print(\"=\" * 60)\n",
    "print(\"BERT 모델 Fine-tuning 시작\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 훈련 결과 저장\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # 훈련\n",
    "    train_loss, train_acc = train_epoch(fine_tune_model, train_loader, optimizer, scheduler, device)\n",
    "    \n",
    "    # 평가\n",
    "    val_loss, val_acc, _, _ = evaluate_model(fine_tune_model, test_loader, device)\n",
    "    \n",
    "    # 결과 저장\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"\\n총 훈련 시간: {training_time:.2f}초 ({training_time/60:.2f}분)\")\n",
    "\n",
    "print(\"\\nFine-tuning 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 평가 및 비교\n",
    "_, final_accuracy, predictions, true_labels = evaluate_model(fine_tune_model, test_loader, device)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"최종 성능 비교\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"기존 방식 (고정 임베딩 + 로지스틱 회귀): {baseline_accuracy:.4f}\")\n",
    "print(f\"Fine-tuning 방식: {final_accuracy:.4f}\")\n",
    "print(f\"성능 향상: {final_accuracy - baseline_accuracy:.4f} ({(final_accuracy - baseline_accuracy)/baseline_accuracy*100:+.1f}%)\")\n",
    "\n",
    "# 분류 보고서\n",
    "print(\"\\n=== Fine-tuned 모델 분류 보고서 ===\")\n",
    "target_names = ['부정', '긍정']\n",
    "print(classification_report(true_labels, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('BERT Fine-tuning 결과 분석', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. 훈련 손실 변화\n",
    "axes[0, 0].plot(range(1, epochs + 1), train_losses, 'b-o', label='Train Loss', linewidth=2)\n",
    "axes[0, 0].plot(range(1, epochs + 1), val_losses, 'r-o', label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_title('Loss 변화', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. 정확도 변화\n",
    "axes[0, 1].plot(range(1, epochs + 1), train_accuracies, 'b-o', label='Train Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(range(1, epochs + 1), val_accuracies, 'r-o', label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_title('Accuracy 변화', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['부정', '긍정'], yticklabels=['부정', '긍정'], ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('예측')\n",
    "axes[1, 0].set_ylabel('실제')\n",
    "\n",
    "# 4. 성능 비교\n",
    "methods = ['기존 방식\\n(고정 임베딩)', 'Fine-tuning\\n방식']\n",
    "accuracies = [baseline_accuracy, final_accuracy]\n",
    "colors = ['lightcoral', 'lightblue']\n",
    "\n",
    "bars = axes[1, 1].bar(methods, accuracies, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_title('성능 비교', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('정확도')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "# 막대 위에 값 표시\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 요약 통계\n",
    "print(\"\\n=== 훈련 요약 ===\")\n",
    "print(f\"최종 훈련 정확도: {train_accuracies[-1]:.4f}\")\n",
    "print(f\"최종 검증 정확도: {val_accuracies[-1]:.4f}\")\n",
    "print(f\"최종 훈련 손실: {train_losses[-1]:.4f}\")\n",
    "print(f\"최종 검증 손실: {val_losses[-1]:.4f}\")\n",
    "print(f\"총 훈련 시간: {training_time:.2f}초\")\n",
    "print(f\"에폭당 평균 시간: {training_time/epochs:.2f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuned 모델로 새로운 텍스트 예측\n",
    "def predict_sentiment_finetuned(text, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Fine-tuned 모델을 사용한 감성 분석 함수\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    token_type_ids = encoding['token_type_ids'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        prediction = torch.argmax(logits, dim=-1).item()\n",
    "        confidence = probabilities[0][prediction].item()\n",
    "    \n",
    "    sentiment = \"긍정\" if prediction == 1 else \"부정\"\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'sentiment': sentiment,\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            '부정': probabilities[0][0].item(),\n",
    "            '긍정': probabilities[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# 테스트 예제\n",
    "test_examples = [\n",
    "    \"이 영화는 정말 재미있었어요! 강력 추천합니다.\",\n",
    "    \"너무 지루하고 시간 낭비였어요.\",\n",
    "    \"배우들의 연기가 훌륭했습니다.\",\n",
    "    \"스토리가 엉성하고 재미없었어요.\",\n",
    "    \"감동적이고 의미 있는 영화였습니다.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Fine-tuned 모델 감성 분석 예제\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(test_examples, 1):\n",
    "    result = predict_sentiment_finetuned(text, fine_tune_model, tokenizer, device)\n",
    "    print(f\"\\n{i}. 텍스트: {result['text']}\")\n",
    "    print(f\"   예측: {result['sentiment']} (신뢰도: {result['confidence']:.4f})\")\n",
    "    print(f\"   확률 - 부정: {result['probabilities']['부정']:.4f}, 긍정: {result['probabilities']['긍정']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "### 기존 방식과 Fine-tuning 비교:\n",
    "\n",
    "**기존 방식 (Movie_Bert.ipynb):**\n",
    "- 고정된 BERT 임베딩 추출\n",
    "- 로지스틱 회귀로 분류\n",
    "- 빠른 실행 시간\n",
    "- 제한적인 성능\n",
    "\n",
    "**Fine-tuning 방식:**\n",
    "- End-to-end 학습\n",
    "- 전체 모델 파라미터 업데이트\n",
    "- 더 긴 훈련 시간\n",
    "- 향상된 성능\n",
    "\n",
    "### 주요 개선사항:\n",
    "1. **성능 향상**: Fine-tuning을 통한 정확도 개선\n",
    "2. **도메인 적응**: 영화 리뷰 데이터에 특화된 모델\n",
    "3. **신뢰도**: 더 높은 예측 신뢰도\n",
    "\n",
    "### 사용 권장사항:\n",
    "- **빠른 프로토타이핑**: 기존 방식 사용\n",
    "- **높은 성능 요구**: Fine-tuning 방식 사용\n",
    "- **실제 서비스**: Fine-tuning 후 모델 배포"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
